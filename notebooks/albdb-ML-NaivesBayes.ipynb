{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_data/hate-speech.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_full = data['text']\n",
    "y_full = data['label']\n",
    "\n",
    "X, _, y, _ = train_test_split(X_full, y_full, test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84752,)\n",
      "(84752,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Key considerations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words**: just the regular list added - possiblity to customize to add or withdraw some words (see below)  \n",
    "**lemmatizing**: you can lemmatize at different levels (nouns, verbs, adjectives, adverbs). I have added all options, but we might consider doing it differently"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleaning(series):\n",
    "    def cleaning_sentence(sentence):\n",
    "        \"\"\" takes a sentence (string) as input and returns\n",
    "        same string but fully cleaned \"\"\"\n",
    "        \n",
    "        # Basic cleaning\n",
    "        sentence = sentence.strip() ## remove whitespaces\n",
    "        sentence = sentence.lower() ## lowercase \n",
    "        sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "        \n",
    "        # Advanced cleaning\n",
    "        for punctuation in string.punctuation:\n",
    "            sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "        \n",
    "        tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "        \n",
    "        stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "        \n",
    "        tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "            w for w in tokenized_sentence if not w in stop_words\n",
    "        ]\n",
    "        \n",
    "        # Lemmatizing\n",
    "        lemmatized_verbs = [WordNetLemmatizer().lemmatize(word, pos = \"v\") for word in tokenized_sentence_cleaned]\n",
    "        lemmatized_nouns = [WordNetLemmatizer().lemmatize(word, pos = \"n\") for word in lemmatized_verbs]\n",
    "        lemmatized_adj = [WordNetLemmatizer().lemmatize(word, pos = \"a\") for word in lemmatized_nouns]\n",
    "        lemmatized_adv = [WordNetLemmatizer().lemmatize(word, pos = \"r\") for word in lemmatized_adj]\n",
    "        \n",
    "        cleaned_sentence = ' '.join(word for word in lemmatized_adv)\n",
    "        \n",
    "        return cleaned_sentence\n",
    "    \n",
    "    return series.apply(cleaning_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom stopwords\n",
    "\"\"\"\n",
    "custom_stop_words = ['custom', 'stop', 'words']\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "# Remove specific stopwords\n",
    "remove_stop_words = ['not', 'no']\n",
    "stop_words.difference_update(remove_stop_words)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cat lovely sky cloud funiest ever\n",
       "1                             time row\n",
       "2          cat dog people unbelievable\n",
       "dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the function with a simple example\n",
    "\n",
    "example = [\"mY Cats are so lovely in the skies with clouds funiest ever.\",\n",
    "           \"i am doing this for the 400 time in row.\",\n",
    "           \"cats dogs people are unbelievable\"]\n",
    "\n",
    "example_series = pd.Series(example)\n",
    "output = cleaning(example_series)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gridsearching the best vectorizer / model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tested multiple things, and appears ngram (1,1) better than (2,2) or (3,3)\n",
    "min df seems to be 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ngram ((1,1), (2,2), (3,3), (2,3)),\\n# min_df: 5, 10, 30, 50\\n# max_df: 0.75, 0.85, 0.95\\n# nalpha: 0.01, 0.1, 1, 10\\n\\n# Create Pipeline\\npipeline = Pipeline([\\n    (\\'tfidf\\', TfidfVectorizer()),\\n    (\\'nb\\', MultinomialNB()),\\n])\\n\\n# Set parameters to search\\nparameters = {\\n    \\'tfidf__ngram_range\\': ((1,1),),\\n    \\'tfidf__min_df\\': (30,30),\\n    \\'tfidf__max_df\\': (0.75,0.8),\\n    \\'nb__alpha\\': (5,10)\\n}\\n\\n# Perform grid search on pipeline\\ngrid_search = GridSearchCV(\\n    pipeline, parameters, n_jobs=-1, \\n    verbose=1, scoring = \"accuracy\", \\n    cv=4\\n)\\n\\ngrid_search.fit(X,y)\\n'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn import set_config; set_config(\"diagram\")\n",
    "\n",
    "\"\"\"\n",
    "# ngram ((1,1), (2,2), (3,3), (2,3)),\n",
    "# min_df: 5, 10, 30, 50\n",
    "# max_df: 0.75, 0.85, 0.95\n",
    "# nalpha: 0.01, 0.1, 1, 10\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Set parameters to search\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1),),\n",
    "    'tfidf__min_df': (30,30),\n",
    "    'tfidf__max_df': (0.75,0.8),\n",
    "    'nb__alpha': (5,10)\n",
    "}\n",
    "\n",
    "# Perform grid search on pipeline\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, parameters, n_jobs=-1, \n",
    "    verbose=1, scoring = \"accuracy\", \n",
    "    cv=4\n",
    ")\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "\"\"\" \n",
    "# to be updated also including the custom cleaning fucntion before vecotriing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best ngram:(1, 1) ; best mindf: 30; best maxdf: 0.75 best npalpha: 5\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = best_model.get_params()\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best ngram:{best_params['tfidf__ngram_range']} ; best mindf: {best_params['tfidf__min_df']}; best maxdf: {best_params['tfidf__max_df']} best npalpha: {best_params['nb__alpha']}\")\n",
    "#wiht an accuracy score of {best_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63564,)\n",
      "(63564,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "cleaner = FunctionTransformer(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=30, max_df=0.8, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-19 {color: black;background-color: white;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function cleaning at 0x7f7ede993be0&gt;)),\n",
       "                (&#x27;tfidfvectorizer&#x27;, TfidfVectorizer(max_df=0.8, min_df=30)),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB(alpha=5))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function cleaning at 0x7f7ede993be0&gt;)),\n",
       "                (&#x27;tfidfvectorizer&#x27;, TfidfVectorizer(max_df=0.8, min_df=30)),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB(alpha=5))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cleaning at 0x7f7ede993be0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.8, min_df=30)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=5)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('functiontransformer',\n",
       "                 FunctionTransformer(func=<function cleaning at 0x7f7ede993be0>)),\n",
       "                ('tfidfvectorizer', TfidfVectorizer(max_df=0.8, min_df=30)),\n",
       "                ('multinomialnb', MultinomialNB(alpha=5))])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(cleaner, vectorizer, model)\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79493109307155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Switched to a septic tank \n",
      "dtype: object\n",
      "0    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_real = pd.Series(data['text'][600])\n",
    "y_real = pd.Series(data['label'][600])\n",
    "print(X_real)\n",
    "print(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_example = pipeline.predict(X_real)\n",
    "predict_example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"pipeline.pkl\",\"wb\") as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "with bz2.BZ2File(\"/home/albane/code/Ancastal/fake-news/raw_data/hate_speech\",'rb') as ifile:\n",
    "    df = pickle.load(ifile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment like pelosi big part reason people tir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>caught meth say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>byron york got exactly correct democrat want d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wish goal animal right activist people strt re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>censored civil gestapo hate left shock else on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339005</th>\n",
       "      <td>clearly view someone never served day branch u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339006</th>\n",
       "      <td>never state project recall city ever asked sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339007</th>\n",
       "      <td>abernathy admire trump attack ag jefferson bea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339008</th>\n",
       "      <td>inept governor puerto rico presidential appoin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339009</th>\n",
       "      <td>presence marxist university campus new year pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>339010 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       comment like pelosi big part reason people tir...      0\n",
       "1                                         caught meth say      0\n",
       "2       byron york got exactly correct democrat want d...      0\n",
       "3       wish goal animal right activist people strt re...      0\n",
       "4       censored civil gestapo hate left shock else on...      0\n",
       "...                                                   ...    ...\n",
       "339005  clearly view someone never served day branch u...      1\n",
       "339006  never state project recall city ever asked sta...      1\n",
       "339007  abernathy admire trump attack ag jefferson bea...      1\n",
       "339008  inept governor puerto rico presidential appoin...      1\n",
       "339009  presence marxist university campus new year pl...      1\n",
       "\n",
       "[339010 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
