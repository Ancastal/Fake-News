{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/antoniocastaldo/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/antoniocastaldo/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/antoniocastaldo/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/antoniocastaldo/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from gensim) (6.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mbib-base (/Users/antoniocastaldo/.cache/huggingface/datasets/mediabiasgroup___mbib-base/hate-speech/1.0.0/cf6f80c612f1363f2162f92f58e1113915a6b01aa07680513a18b7d94570e875)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29292d301c91452db783dfbe8c4e4bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_wiki = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "dataset = load_dataset(\"mediabiasgroup/mbib-base\", \"hate-speech\")\n",
    "df = dataset['train'].to_pandas()\n",
    "df = df.sample(frac=0.2).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    removed_stopwords = ' '.join([word for word in sentence.split() if word not in stopwords])\n",
    "    \n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = removed_stopwords.replace(punctuation, '') \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(basic_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "\n",
    "    return embed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(model_wiki, X_train)\n",
    "X_test_embed_2 = embedding(model_wiki, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.LSTM(20))\n",
    "    model.add(layers.Dense(10, activation='tahn'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Starting padding X_train_pad_2 \n",
      "âœ… Completed padding X_train_pad_2 \n",
      "ðŸ‘‰ Starting padding X_test_pad_2 \n",
      "âœ… Completed padding X_test_pad_2 \n",
      "\n",
      "ðŸ‘‰ Starting training...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 12:57:11.302834: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6781/6781 [==============================] - 135s 20ms/step - loss: 0.6906 - accuracy: 0.5295 - val_loss: 0.6890 - val_accuracy: 0.5313\n",
      "Epoch 2/50\n",
      "6781/6781 [==============================] - 131s 19ms/step - loss: 0.6894 - accuracy: 0.5320 - val_loss: 0.6896 - val_accuracy: 0.5314\n",
      "Epoch 3/50\n",
      "6781/6781 [==============================] - 133s 20ms/step - loss: 0.6875 - accuracy: 0.5383 - val_loss: 0.6869 - val_accuracy: 0.5369\n",
      "Epoch 4/50\n",
      "6781/6781 [==============================] - 132s 19ms/step - loss: 0.6821 - accuracy: 0.5527 - val_loss: 0.6855 - val_accuracy: 0.5409\n",
      "Epoch 5/50\n",
      "6781/6781 [==============================] - 132s 19ms/step - loss: 0.6748 - accuracy: 0.5520 - val_loss: 0.6727 - val_accuracy: 0.5464\n",
      "Epoch 6/50\n",
      "6781/6781 [==============================] - 132s 19ms/step - loss: 0.6086 - accuracy: 0.6508 - val_loss: 0.5796 - val_accuracy: 0.6825\n",
      "Epoch 7/50\n",
      "6781/6781 [==============================] - 132s 19ms/step - loss: 0.5577 - accuracy: 0.7045 - val_loss: 0.5396 - val_accuracy: 0.7227\n",
      "Epoch 8/50\n",
      "6781/6781 [==============================] - 181s 27ms/step - loss: 0.5278 - accuracy: 0.7328 - val_loss: 0.5265 - val_accuracy: 0.7329\n",
      "Epoch 9/50\n",
      "6781/6781 [==============================] - 164s 24ms/step - loss: 0.4999 - accuracy: 0.7550 - val_loss: 0.4827 - val_accuracy: 0.7706\n",
      "Epoch 10/50\n",
      "6781/6781 [==============================] - 339s 50ms/step - loss: 0.4741 - accuracy: 0.7764 - val_loss: 0.4659 - val_accuracy: 0.7844\n",
      "Epoch 11/50\n",
      "6781/6781 [==============================] - 663s 98ms/step - loss: 0.4611 - accuracy: 0.7861 - val_loss: 0.4554 - val_accuracy: 0.7922\n",
      "Epoch 12/50\n",
      "6781/6781 [==============================] - 552s 81ms/step - loss: 0.4512 - accuracy: 0.7928 - val_loss: 0.4472 - val_accuracy: 0.7962\n",
      "Epoch 13/50\n",
      "6781/6781 [==============================] - 128s 19ms/step - loss: 0.4435 - accuracy: 0.7994 - val_loss: 0.4392 - val_accuracy: 0.8030\n",
      "Epoch 14/50\n",
      "6781/6781 [==============================] - 185s 27ms/step - loss: 0.4374 - accuracy: 0.8036 - val_loss: 0.4368 - val_accuracy: 0.8042\n",
      "Epoch 15/50\n",
      "6781/6781 [==============================] - 135s 20ms/step - loss: 0.4330 - accuracy: 0.8066 - val_loss: 0.4337 - val_accuracy: 0.8076\n",
      "Epoch 16/50\n",
      "6781/6781 [==============================] - 137s 20ms/step - loss: 0.4291 - accuracy: 0.8089 - val_loss: 0.4329 - val_accuracy: 0.8071\n",
      "Epoch 17/50\n",
      "6781/6781 [==============================] - 137s 20ms/step - loss: 0.4261 - accuracy: 0.8110 - val_loss: 0.4282 - val_accuracy: 0.8098\n",
      "Epoch 18/50\n",
      "6781/6781 [==============================] - 137s 20ms/step - loss: 0.4242 - accuracy: 0.8122 - val_loss: 0.4252 - val_accuracy: 0.8122\n",
      "Epoch 19/50\n",
      "6781/6781 [==============================] - 135s 20ms/step - loss: 0.4215 - accuracy: 0.8132 - val_loss: 0.4293 - val_accuracy: 0.8098\n",
      "Epoch 20/50\n",
      "6781/6781 [==============================] - 133s 20ms/step - loss: 0.4193 - accuracy: 0.8148 - val_loss: 0.4223 - val_accuracy: 0.8139\n",
      "Epoch 21/50\n",
      "6781/6781 [==============================] - 137s 20ms/step - loss: 0.4179 - accuracy: 0.8154 - val_loss: 0.4294 - val_accuracy: 0.8091\n",
      "Epoch 22/50\n",
      "6781/6781 [==============================] - 135s 20ms/step - loss: 0.4161 - accuracy: 0.8172 - val_loss: 0.4221 - val_accuracy: 0.8137\n",
      "Epoch 23/50\n",
      "6781/6781 [==============================] - 136s 20ms/step - loss: 0.4148 - accuracy: 0.8170 - val_loss: 0.4236 - val_accuracy: 0.8144\n",
      "Epoch 24/50\n",
      "6781/6781 [==============================] - 134s 20ms/step - loss: 0.4135 - accuracy: 0.8179 - val_loss: 0.4243 - val_accuracy: 0.8120\n",
      "Epoch 25/50\n",
      "6781/6781 [==============================] - 133s 20ms/step - loss: 0.4129 - accuracy: 0.8186 - val_loss: 0.4170 - val_accuracy: 0.8162\n",
      "Epoch 26/50\n",
      "6781/6781 [==============================] - 134s 20ms/step - loss: 0.4330 - accuracy: 0.8054 - val_loss: 0.4540 - val_accuracy: 0.7966\n",
      "Epoch 27/50\n",
      "6781/6781 [==============================] - 136s 20ms/step - loss: 0.4378 - accuracy: 0.8041 - val_loss: 0.4367 - val_accuracy: 0.8053\n",
      "Epoch 28/50\n",
      "6781/6781 [==============================] - 135s 20ms/step - loss: 0.4144 - accuracy: 0.8179 - val_loss: 0.4272 - val_accuracy: 0.8114\n",
      "Epoch 29/50\n",
      "6781/6781 [==============================] - 131s 19ms/step - loss: 0.4109 - accuracy: 0.8202 - val_loss: 0.4183 - val_accuracy: 0.8158\n",
      "âœ… Completed training\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ‘‰ Starting padding X_train_pad_2 \")\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2,\n",
    "                              dtype='float32',\n",
    "                              padding='post',\n",
    "                              maxlen=200)\n",
    "print(f\"âœ… Completed padding X_train_pad_2 \")\n",
    "print(f\"ðŸ‘‰ Starting padding X_test_pad_2 \")\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2,\n",
    "                             dtype='float32',\n",
    "                             padding='post',\n",
    "                             maxlen=200)\n",
    "print(f\"âœ… Completed padding X_test_pad_2 \")\n",
    "print(\"\")\n",
    "print(\"ðŸ‘‰ Starting training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "model.fit(X_train_pad_2,\n",
    "          y_train,\n",
    "          epochs=50,\n",
    "          batch_size=32,\n",
    "          verbose=1,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[EarlyStopping(patience=4, restore_best_weights=True)])\n",
    "print(\"âœ… Completed training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 81.748%\n",
      "The accuracy of the baseline model is of 49.978%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "baseline_model = DummyClassifier(strategy='most_frequent')\n",
    "baseline_model.fit(X_train_pad_2, y_train)\n",
    "baseline_model.score(X_test_pad_2, y_test)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "print(f'The accuracy of the baseline model is of {baseline_model.score(X_test_pad_2, y_test)*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NN accuracy is 31.77% higher than the baseline model\n"
     ]
    }
   ],
   "source": [
    "print('The NN accuracy is {:.2f}% higher than the baseline model'.format((res[1] - baseline_model.score(X_test_pad_2, y_test))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "2119/2119 [==============================] - 10s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83     33886\n",
      "           1       0.85      0.77      0.81     33916\n",
      "\n",
      "    accuracy                           0.82     67802\n",
      "   macro avg       0.82      0.82      0.82     67802\n",
      "weighted avg       0.82      0.82      0.82     67802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:')\n",
    "y_pred = model.predict(X_test_pad_2)\n",
    "print(classification_report(y_test, y_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20)                5680      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                210       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,901\n",
      "Trainable params: 5,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# class balance\n",
    "model.save('model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
