{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_racial = pd.read_csv(\"../raw_data/racial-bias.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_racial['text']\n",
    "y = data_racial['label']\n",
    "    \n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def cleaning(series):\n",
    "    def cleaning_sentence(sentence):\n",
    "        \"\"\" takes a sentence (string) as input and returns\n",
    "        same string but fully cleaned \"\"\"\n",
    "        \n",
    "        # Basic cleaning\n",
    "        sentence = sentence.strip() ## remove whitespaces\n",
    "        sentence = sentence.lower() ## lowercase \n",
    "        sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "        \n",
    "        # Advanced cleaning\n",
    "        for punctuation in string.punctuation:\n",
    "            sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "        \n",
    "        tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "        \n",
    "        stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "        \n",
    "        tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "            w for w in tokenized_sentence if not w in stop_words\n",
    "        ]\n",
    "        \n",
    "        # Lemmatizing\n",
    "        lemmatized_verbs = [WordNetLemmatizer().lemmatize(word, pos = \"v\") for word in tokenized_sentence_cleaned]\n",
    "        lemmatized_nouns = [WordNetLemmatizer().lemmatize(word, pos = \"n\") for word in lemmatized_verbs]\n",
    "        lemmatized_adj = [WordNetLemmatizer().lemmatize(word, pos = \"a\") for word in lemmatized_nouns]\n",
    "        lemmatized_adv = [WordNetLemmatizer().lemmatize(word, pos = \"r\") for word in lemmatized_adj]\n",
    "        \n",
    "        cleaned_sentence = ' '.join(word for word in lemmatized_adv)\n",
    "        \n",
    "        return cleaned_sentence\n",
    "    \n",
    "    return series.apply(cleaning_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "cleaner = FunctionTransformer(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_df=0.7, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "class CustomVotingClassifier(VotingClassifier):\n",
    "    def predict(self, X):\n",
    "        # Get the individual model predictions\n",
    "        predictions = super().predict(X)\n",
    "\n",
    "        # Check if any model predicts 1\n",
    "        if any(prediction == 1 for prediction in predictions):\n",
    "            # Set all predictions to 1\n",
    "            predictions = [1] * len(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# Define the base classification models\n",
    "gboost = GradientBoostingClassifier(n_estimators=100)\n",
    "logreg = LogisticRegression(solver='liblinear', C=0.1)\n",
    "adaboost = AdaBoostClassifier()\n",
    "\n",
    "# Create the ensemble classifier\n",
    "model = VotingClassifier(\n",
    "    estimators=[(\"gboost\", gboost), (\"adaboost\", adaboost), (\"logreg\", logreg)],\n",
    "    voting='soft',  # Use soft voting for probabilistic classification models\n",
    "    weights=[1, 1, 1],  # Equal weights for all models\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_ensemble = make_pipeline(cleaner, vectorizer, model)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "probabilities = pipe_ensemble.predict_proba(X_test)\n",
    "threshold = 0.55  # Adjust the threshold as needed\n",
    "class_predictions = (probabilities[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the performance of the ensemble model\n",
    "accuracy = accuracy_score(y_test, class_predictions)\n",
    "recall = recall_score(y_test, class_predictions)\n",
    "precision = precision_score(y_test, class_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6885982836125868\n",
      "0.5059021922428331\n",
      "0.7731958762886598\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the base classification models\n",
    "gboost = GradientBoostingClassifier(n_estimators=100)\n",
    "logreg = LogisticRegression(solver='liblinear',C=0.1)\n",
    "adaboost = AdaBoostClassifier()\n",
    "randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create the ensemble classifier\n",
    "model = VotingClassifier(\n",
    "    estimators=[(\"gboost\", gboost), (\"adaboost\", adaboost),(\"logreg\", logreg),(\"randomforest\",randomforest)],\n",
    "    voting='soft',  # Use soft voting for probabilistic classification models\n",
    "    weights=[1, 1, 1],  # Equal weights for all models\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_ensemble = make_pipeline(cleaner, vectorizer, model)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipe_ensemble.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred )\n",
    "precision= precision_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4846751123825092\n",
      "1.0\n",
      "0.4846751123825092\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# Define the base classification models\n",
    "gboost = GradientBoostingClassifier(n_estimators=100)\n",
    "logreg = LogisticRegression(solver='liblinear', C=0.1)\n",
    "adaboost = AdaBoostClassifier()\n",
    "randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create the ensemble classifier with customized voting\n",
    "class CustomVotingClassifier(VotingClassifier):\n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        class_predictions = []\n",
    "        for probas in probabilities:\n",
    "            if (probas >= 0.55).sum() >= 1:  # If at least two models predict 1\n",
    "                class_predictions.append(1)\n",
    "            else:\n",
    "                class_predictions.append(0)\n",
    "        return class_predictions\n",
    "\n",
    "# Create the customized ensemble classifier\n",
    "model = CustomVotingClassifier(\n",
    "    estimators=[(\"gboost\", gboost), (\"adaboost\", adaboost), (\"logreg\", logreg), (\"randomforest\", randomforest)],\n",
    "    voting='soft',  # Use soft voting for probabilistic classification models\n",
    "    weights=[1, 1, 1, 1],  # Equal weights for all models\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipe_ensemble = make_pipeline(cleaner, vectorizer, model)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipe_ensemble.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5026563138536984\n",
      "0.7495784148397976\n",
      "0.49143173023770037\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
